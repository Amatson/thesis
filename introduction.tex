%!TEX root = thesis.tex
%% %% ***************** Introduction *****************

\section{Introduction}\label{sec:introduction}

%% Leave page number of the first page empty
\thispagestyle{empty}
%% TODO: Remember references
Artificial intelligence (AI) and machine learning (ML)
has found their way to
more and more fields of business.
In banking business they are already used in
fraud detection, risk management and service recommendations.\cite{donepudi2017machine}
Even though these
modern big data utilizing technologies
are widely used abroad,
in Finnish banking field AI and ML are not popularly utilized.
Instead,
many self-acting solutions are being used
to streamline manual labor
which could be called intelligent,
but are merely highly automated processes
and thus cannot be included in the AI category.
One of these technologies used in Finnish banking systems
is Robotic Process Automation (RPA).
%% TODO: Should IPA be mentioned (Intelligent Process Automation) and in what level?

RPA operates \enquote{on the user interface of other computer systems
in the way a human would do},\cite{van2018robotic}
but is strictly bounded by predefined operations
thus being prone to unforeseen situations
such as faulty input.
RPA, like generally all other software,
produces log to \enquote{register
the automatically produced and time-stamped documentation
of events, behaviors and conditions
relevant to a particular system}\cite{delarosa2018log}.
Logs don't have any standards or form guidelines to follow
which tends to make
log analysis and log based problem-solving troublesome.
This is also the case with RPA's developed by Oy Samlink Ab.

Oy Samlink Ab (Samlink from now on)
was founded in 1994
and is now owned by Kyndryl.
From the early years
while going by the name of Samcom
the company was owned by several Finnish banks
developing all sorts IT solutions for them.
Nowadays,
Samlink offers a wide variety of banking solutions
from basic banking system to Codeapp-mobile software.

Besides banking,
Samlink develops multiple other IT solutions
to extensive range of customers,
for example,
entertainment platform solutions for DNA\@.
Even though Samlink can be considered
a modern technology company,
the most modern AI technologies has not yet been adopted
in the variety of tools used in development.
However,
RPA has been actively used
in some banking solutions
to reduce the amount of manual labor required
from banking clerks.

In addition to continuous development
as well as product maintenance services,
Samlink also offers a technical help desk
regarding the software solutions produced.
As no IT solutions comes without bugs or misbehaviour,
Samlink service desk has to use considerable amount of labor
to resolve if technical support request tickets received
are due to the problems in programming
and require fixing in production.
In many cases,
the problem-solving starts by reading the log
and analyzing the data written by processes in question.

In this study,
we aim to find if it is possible to utilize
ML methods in analyzing logs
created by Samlink RPA's.
%% TODO: Check terms! RPA-robots? RPA-processes? How to define term hierarchy?
Ultimately,
we intend to train ML
which is able to predict the arrival of a technical support ticket
thus giving a warning for developers about possible issues
in the production.

%% TODO: What creates logs?

%% ************************************************************************************************************

\subsection{Background and motivation}\label{subsec:intro-background-and-motivation}
In the field of information technology
logging is one of the most important methods
in problem-solving,
be it software or operating system related.\cite{delarosa2018log}
Typically,
at least in Samlink processes,
logging is a bit more verbose
than it needs to be.
This is usually because when the problem occurs
it is easier to already have the verbose logs available
than trying to replicate the issue
after setting logging to more verbose mode.
Too verbose logging, however,
leads into two problematic issues for Samlink.

First of, the size of log is huge
and finding the critical lead for
solving the problem in hand
takes more time.
Of course,
with more strict logging
pinpointing the issue from within the logs
would be faster,
but then again,
solving the problem with only critical error messages
could be more time-consuming
if crucial context is missing.

Secondly,
a well-designed software
is able to retry the process after first failure,
but logging is done in real time,
not after final results of the process has been determined.
This means,
that each process failure is logged
even though said process succeeds eventually.
Thus,
logs may include dozens of rows of information about a problem encountered,
which are not critical information after all.
These issues make log analyzing considerably laborious.

Production logs are usually not viewed
if everything is presumably working as intended.
Technical support tickets are both
last and most visible indicator
that something is wrong.
When a technical support ticket is received from banking clerks
it means that something is wrong
in a very visible way.
Roughly speaking,
technical tickets that are due to
clear misbehavior of the RPA systems
and not, for example, user errors,
can be divided in two categories.
First are the tickets
that uncover an unknown bug in the system
which can be either fixed
or instructed to user how to avoid.
Second type of tickets are
somewhat pre-known issues
that occur from time to time
and are either fixed with updating parts of the system
or by rebooting the process.

Typically,
in software systems,
if issue is known and can be fixed by rebooting something,
developers can create log monitors
that search for certain keywords
and raise an alert if encountered.
However,
when it comes to RPA's and technical tickets considering them,
it hard to say which type of issue is in question
by reading the RPA logs word by word without context.
New kind of issues can be more frequent
than already confronted ones,
and clear keyword linked to
a certain problem may not exist
without considerable amount of false positive matches.

ML algorithms are widely used
to find patterns from massive amount of data
making it an ideal tool for log analyzing.
Patterns, however,
need a connection to a visible issue.
If RPA system has encountered an error
but is able to retry successfully,
then no issue has practically happened
that needs immediate concern.
Hence,
RPA log analyzing with ML can find meaningful patterns
only if they relate to actual technical tickets.

If Samlink support has received a help request
the issue behind the request is not fresh anymore.
In the event of RPA job failing,
it takes some time for the clerk to notice the issue,
write a help request to first support level,
which then redirects the ticket to corresponding team.
Furthermore,
if the issue is noticed during friday,
it takes few more days to be handled by RPA developers.
This leads to noticeable delay in processes
that were supposed to be dealt by RPA
but which now have to be manually taken care of
by said clerks.

%% TODO:
\todo{Add something about why ticket forecasting would be desirable}

\todo{something about log format?}

\toimhuom{Otherwise, this chapter is good to go!}

%% ************************************************************************************************************

\subsection{Research objectives}\label{subsec:intro-research-objectives}
This research aims to kickstart the ML application usage in Samlink operations.
%% TODO: This cannot be said right now as Samlink is part of Kyndryl
Multiple obstacles need to be tackled
as most of the phases in this study
has not yet been encountered inside the company.

First and foremost,
it is crucial to construct some basic form for log data
to make it usable by ML algorithms.
Log data forming is one of the key elements in automatic log analyzing applications
as it is not for just machines
but also for people to read.

As today more and more concern is set on anonymization
not only due to GDPR,
the data used for machine learning must be sanitized.
Because of this,
one major objective is to create a clean dataset
that is safe to use in cloud environment
without raising concern around security and privacy issues.
In addition to this,
data must also be clean enough
so that ML algorithms
are able to process it.

Finally,
the main objective this research is aiming to answer
is whether it is possible to use ML algorithms
in such ways that combine support ticket timestamps
and software run log
so that an algorithm can predict
if new support ticket might be coming.

\begin{verbatim}
  * Counting anomaly "probability" for individual log rows
  * Random delay! Solving issue by grouping
  * Hybrid ML with anomaly detection and regression
  => Can this give any results for forecasting?
  ?? Assumption to lean on:
    !! Ticket causing log rows are ANOMALIES in log data.
    TODO: explain why this assumption is necessary!!!
    - Log amount is not necessarily proportional to ticket creating error amount, even with info-types filtered out
    - Considerable amount of rows may be related to one, or several different, tickets
\end{verbatim}

What really happens in this research:
\begin{verbatim}
<Anonymization scope>
<Azure ML studio setup and prerequisites>
<Log data and timestamp combining>
<Connection for data, ML estimates>
\end{verbatim}

%% ************************************************************************************************************

\subsection{Scope}\label{subsec:intro-scope}

In order to limit the study
to feasible length and content,
it is necessary to define the scope for the thesis.
As major part of the time consumed for the study
was used for data acquiring and anonymizing
yet the most value comes from
content considering machine learning,
the length of the content per section
does not reflect
the amount of time used for each section.

Before diving in to the scope of research objectives,
we must first make one assumption regarding the data
that ML is going to find some meaning from.
In order to find a connection between log data and support tickets,
we make a hypothesis that errors leading to tickets
are visible to or parseable by ML algorithm.
In addition,
as we are going to utilize anomaly detection algorithm in log analyzing,
we must also assume that these errors in log
are, in fact, anomalies.
We will, however,
compare the results against these assumptions
to test this hypothesis.

%% ************************************************************************************************************

\subsubsection*{Data anonymization}
Anonymization in the context of this thesis
refers to data sanitization process
purposed to edit the data into
more secure form in the privacy point of view.
In this study we aim to
create a dataset usable in ML training.
In this respect
anonymization is not in the main focus of the study
but only treated as a sub-phase
of the data preprocessing in whole.
Nevertheless,
anonymization is from the privacy point of view
the most important phase of data preprocessing.

Keeping this in mind,
anonymization is covered rather superficially,
only enough to explain the reasons
behind actions taken during anonymization process.

%% ************************************************************************************************************

\subsubsection*{Azure setup}
The ML training and result scoring
is done in Azure ML environment.
Azure is used because... %% TODO: Justify reasons behind selecting Azure ML Studio
As part of the study aims to create
an initial guideline for
ML process commissioning
the Azure setup phase is documented in such detail
reflecting the importance of this information
for future developers
starting Azure ML projects in Samlink.

\todo{Maybe something more}




%% ************************************************************************************************************

\subsubsection*{Data requirements}
Data purity in a sense of
how easy it is to be used by ML algorithms
creates challenges at the beginning of ML training.
If data is not consistent,
has lots of missing values
or is formed in unanticipated way,
it requires considerable amount of preprocessing
slowing the training process
and causing errors in pipeline runs.

In order to create a baseline for Samlink ML projects
this study aims to give basic criterion
what is required from the data,
so it is easily analyzable by ML algorithm.

%% ************************************************************************************************************

\subsubsection*{Machine learning methods}
Several different machine learning algorithms exist
that are aimed for different applications in mind.
For example,
to make an algorithm that can predict
the price of an apartment listed\cite{winky}
we could be using linear regression,
and in order to detect
possible cyber threats from network traffic\cite{ghanem}
a two-class support vector machine could be utilized.
These two methods are very different in usage
and has their pros and cons in different applications.

As different methods can be used in creative ways
in very different applications
depending on how the data is presented
and how the ML problem is formed,
this study focuses on
just a few easily approachable training methods.

When it comes to anomaly detection,
only principal component analysis (PCA)
is considered because
PCA-based Anomaly Detection component is the only one usable
from existing anomaly detection algorithms
in Azure ML Studio.
As purely Azure ML Studio is used during this study,
no other anomaly detection algorithms are debated.

%% TODO: maybe mention some other ADA-algorithms that exist even though not usable in Azure?

%% ************************************************************************************************************

\subsection{Structure}\label{subsec:intro-structure}

\todo{Open up what we are going to discuss here and in what order}

\clearpage


%% TODO! Use "Machine Learning" instead of "ML" in more places! Find relevant places to use!