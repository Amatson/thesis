%!TEX root = thesis.tex
%% %% ***************** Results *****************


\section{Results}\label{sec:results}

%% TODO: preface for the section
%We start this section by looking at
%the Azure resources needed for ML training
%both in general Azure environment and in ML Studio.
%Next we analyze the results of
%different ML algorithms and pipelines.

%% ************************************************************************************************************


\todo{Contents of this section has been reorganized to methods and pipeline -sections. Final results are still gathered. Below is some quickly written notes, subjected to change.}

Even though with rawmessage we were forced to reduce the data amount significantly,
it was seen as valuable data source
as it included more data than pure message had in JSON structure.
As data amount was skimmed to less than 1\% of the original amount,
the results may not be reliable.
However, with rawmessage preprocessing used
with decision forest regression,
the coefficient of determination was over 0.35.

\begin{table}[]
    \begin{tabularx}{\textwidth}{|Y|L{0.12\textwidth}|L{0.12\textwidth}|L{0.12\textwidth}|L{0.12\textwidth}|L{0.12\textwidth}|}
        \hline
        \textbf{Algorithm} &
        \textbf{Mean Absolute Error} &
        \textbf{Root Mean Squared Error} &
        \textbf{Relative Squared Error} &
        \textbf{Relative Absolute Error} &
        \textbf{Coefficient of Determination} \\ \hline
        Rawmsg, unconventional training                          & 3.5      & 4.355313 & 0.890083 & 0.924528 & 0.109917 \\ \hline
        Rawmsg, unconventional training, anomaly values excluded & 3.544643 & 4.204987 & 0.8297   & 0.936321 & 0.1703   \\ \hline
        Rawmsg, proper training                                  & 2.848214 & 3.696191 & 0.641063 & 0.752358 & 0.358937 \\ \hline
        Rawmsg, proper training, anomaly values excluded         & 3.133929 & 4.025449 & 0.760362 & 0.82783  & 0.239638 \\ \hline
    \end{tabularx}
    \caption{Rawmessage with text preprocessed, Decision Forest Regresssion used in phase 2}
    \label{tab:results-raw-message}
\end{table}


%% ************************************************************************************************************


%% TODO: Discuss about rawmessage memory issues in result section!


\subsection{Memory issues and limitations}\label{subsec:pipe-memory-issues}
Memory is crucial resource in ML training.
Algorithms take multiple steps while iterating the data
and intermediate results are stored in the RAM rather than on the disk.
%% TODO: something more to say or back that up?
While building ML pipeline in Azure ML studio,
a memory issue emerged
that affected several components
and caused serious limitations
in terms of usable components and data size.
Due to the time limits of this study
this issue was not resolved
and the problem behind it was not found.
As several conditions
considering the environment costs
were already issued by the company,
the issue was supposed to be linked with
compute instance property limitations
and thus could not be resolved without cost overruns.
However, this was not certain.

\todo{Limitations to components. List effects of this problem.}
%% TODO: limitations to components
%% limitations to data size
%% effects on choises in components?




During the initial pipeline runs
the execution came to an abrupt stop
and Azure notified about memory issues. %% TODO: Add reference to issue
These problems were linked to the data amount
which had to be reduced to 600 megabytes
before any pipeline could be finished using the data.
This reduction was against the initial goal
where preferably all the data could have been used.

Considerable amount of time was used
to fix or avoid this issue
but nothing clear was found
that would explain the error received.
While working with the issue
it was also noted
that data needed more cleaning
in order to ease the preprocessing phase
as described with more detail in section~\ref{subsec:meth-data-anonymization}
Thus,
the data had to be imported from log archive
and anonymized once more.

To advance the study more efficiently
it was decided to trim info-type log messages from data
hence reducing the data amount considerably.
Final data included 8.6 million log rows
which was about 10\% of the original data size.
Before final cleaning operations
the data took 8.1GB od disk space,
and after cleaning the rawmessage-field
the final disk size was 6.6GB\@.
Even with this data size,
some Azure ML components faced this memory issue
and forced us to choose such components
that were able to handle these data amounts.

%% TODO: Move some parts of the above memory problem elsewhere


\clearpage