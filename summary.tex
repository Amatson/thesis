%!TEX root = thesis.tex
%% %% ***************** Summary *****************

\section{Summary}\label{sec:summary}
\todo{Very under construction...}

<Sum up here what we did and why>

\subsection{Discussion}\label{subsec:discussion}
<Here some thinking what should have been improved>

Integrating with real time logging?

\subsubsection*{Data formatting}
The most time-consuming tasks in the study
was without a doubt
the anonymization and preformatting of the data.
Although sensitive information may sometimes be crucial in error fixing
as problems may consider just one client,
it is necessary that the data sanitation is possible to do
in order to use the data in less secure environment.
By preformatting the data in such way
that all different personal information types
do not differ between use cases.
%% TODO: hetu in weird form


\subsubsection*{Possible ML methods}
Some sort of time delay forecasting?\cite{erharter2021pointlessness}
Could we estimate the number of tickets
based on some log metrics in time frame?

Memory error fixing would have given more options.

\todo{Something else specifically needed?}

If log would have been better organized/preformatted
it would have been possible to use One-Class Support Vector Machine.
This, however,
would have needed a manually constructed dataset
including only "normal" log events
or such events that could have been certain of
that they were not part of the ticket inducing issues.

%% TODO: Shifting timeframes from mon-sun -> sat-fri ?!?!
\todo{Shifting timeframes from mon-sun -> sat-fri }

Some more testing would be needed to determine
whether splitting data randomly or not in the phase 1
provides better results.
By splitting data randomly
it is possible to miss important anomalous rows that come in groups
which would provide necessary insight to identify anomalous events.

%% TODO:
\todo{Show some explaining graphics, here or elsewhere.}


The original hypothesis was
that anomalous events in the logs
were clearly linked to the tickets received.
However,
as memory errors due to the size of data forced us to skip info-typed rows,
it is possible the data anomalies did not reflect to the tickets.
As stated before,
multiple error lines in the log may be linked to a single issue,
which could make the ticket inducing events
more common log feature.
Thus,
by tuning the statistical values used
to take into account more common error messages,
it could have been possible to get better results.

%% TODO:
\todo{Could anomaly probability metrics be calculated per job ID?}


\clearpage