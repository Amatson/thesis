%!TEX root = thesis.tex
%% %% ***************** Background *****************

%% ************************************************ 2 ************************************************

\section{Background}\label{sec:background}
%\section{Aikaisempi tutkimus}

Machine learning, or ML,
is a subcategory of the AI field and data science.
Typically, ML refers to
a set of technologies used to \enquote{build computers
that improve automatically through experience}.~\cite{jordan2015machine}
This is generally considered a machine way
to simulate human learning process.
ML usage has become more common
and is nowadays widely used in many fields,
not just in general information technology and computer science.
This is because data can be gathered from anywhere,
and where there is data to be processed,
ML can be there to process it.
Computer algorithms are able to find 
statistical correlation and patterns
from places overlooked by human mind,
or where amount of data is just too much 
for people to process.
This is why ML has proved its power
in various empirical science fields,
such as biology, cosmology or social science.~\cite{jordan2015machine}

In this section, 
key concepts of ML are explained briefly
and several ML features are explored 
that are most relevant to this study.
We also discuss shortly about data sensitivity
and how it had to be addressed during this study.

%% ************************************************************************************************************


\subsection{Machine learning algorithms and training}\label{subsec:bg-machine-learning}

Algorithm means a finite sequence of (typically) mathematical operations
that are used to solve a specific problem,
generally by repetition of some steps
until the problem resolves.~\cite{merriam2022algorithm}
Algorithms are the main component inside machine learning~
By iterating through all the data points
algorithm is able to, for example,
find repeating patterns,
mathematical or logical connections,
or unusual anomalies that would be seemingly normal for human eye.

Algorithms operate on set of rules and parameters
In order to utilize an algorithm to solve a problem,
algorithm is first trained by tuning these parameters
to fit the current case.
Usually,
ML algorithms can be trained in three ways:
supervised, unsupervised, and reinforced learning.~\cite{jordan2015machine}
Even more training methods exist
that usually combine those mentioned.~\cite{ayodele2010types, mahesh2020machine}
For the sake of simplicity,
we focus on those three main methods.

In \textbf{supervised learning},
algorithm is given data with ready answers on
how the data needs to be interpreted.
Algorithm then tries to figure out the rules behind
how given data and the correct answers are related.~\cite{ayodele2010types}
In \textbf{unsupervised learning},
on the other hand,
algorithm does not get model data from which to train itself,
but instead it tries to find clusters or groups inside the data
that are linked together more closely than to other data points.~\cite{winky}
\textbf{Reinforced learning} refers to a method
where a computer program is given a goal
and provided feedback as a reward.
This reward is what program aims to maximize
by adjusting given parameters.~\cite{ayodele2010types}

In ML,
there are multiple algorithms to solve different problems
and no jack-of-all-trades algorithm exists.
Each algorithm is suitable for certain type of problem.
To simplify,
algorithms are usually divided into three or four categories
based on the problem type.~\cite{vickery2019mltypes}

\textbf{Regression algorithms} predict values
and are typically used with supervised learning.
Usual example of regression problem
is house price prediction
using typical house features
such as building
year,location, number of rooms \etc.
With these varying features
the algorithm then gives each feature a weight value
which determine the final price of the house.~\cite{vickery2019mltypes}

\textbf{Classification algorithms} predict categories
and are also used most commonly with supervised learning.
Depending on the algorithm,
they can predict between two or several categories.
Examples of classification problems
could be spam mail identification with two class classification,
or flower species recognition from images with multiclass classification.~\cite{vickery2019mltypes}

\textbf{Clustering algorithms} use unsupervised learning
to find structures inside data.
This is done,
for instance,
by first providing the amount of clusters to search to algorithm,
which then calculates a center point for each cluster
so that they are as far away from each other as possible
while data points surrounding each center are as close to each other as possible.~\cite{mahesh2020machine}
This could be used,
for example,
to find meaningful customer segments from transaction data
in order to improve targeted advertising.~\cite{chen2017purtreeclust}

\textbf{Dimension reduction algorithms} are a separate type of algorithms used with unsupervised learning,
but they are usually combined with other algorithms
to solve the main problem.
With dimension reduction,
main algorithm calculations are streamlined by first reducing the amount of feature dimensions.~\cite{li2017mlalgorithm}

These four ML problem types
and most known algorithms of each type
are shown in the graphic~\ref{fig:ml-algorithm-cheatsheet}.


\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/machine-learning-cheet-sheet-2}
    \caption{Machine learning cheatsheet for algorithm choosing\cite{li2017mlalgorithm}
    \label{fig:ml-algorithm-cheatsheet}}
\end{figure}

This study focuses on anomaly detection,
which, roughly simplified, is a clustering problem
where anomalies are rare incidents outside common clusters.
However,
in this study we utilize PCA-based anomaly detection algorithm,
where PCA refers to Principal Component Analysis,
and which is a dimension reduction algorithm.~\cite{li2017mlalgorithm}
More about PCA
is discussed later in this section.
In addition,
we aim to find a connection between anomalies and incident tickets
by their amount in a timeframe,
which makes the topic in the end a regression problem.

Typically,
when training an algorithm,
some predefined portion of the data
is used as training data.
The rest is used to validate the results
so that validation data and training data do not overlap.
Instead, trained algorithm is given data it has not seen before
and the result it produces with it is then validated.~\cite{baheti2022datasplit}
For example,
in supervised learning
the key values the algorithm is trained to find out
are hidden from the validation data.
The resulting values produced by the algorithm
are compared to those hidden values
and the difference between the estimate and the real value
can be used to determine how well the current trained algorithm compares to others.
However, in this study,
we are going to break that rule
about non-overlapping training and validation data.
The reason for this is explained further in section~\ref{subsec:pipe-unconventional-training}.

%% ************************************************************************************************************

\subsection{Cloud ML platforms}\label{subsec:bg-cloud-ml-platforms}

Machine learning algorithms are not light to operate.
ML is at its best with big data
where amount of data points
makes it easier for algorithms
to find repeating patterns more reliably.~\cite{zhou2017machine}
Data amount, however,
requires huge resources in terms of memory and computing power.
Especially with online applications
where real time analysis of new input data is required
with small latency,
cloud computing can make a big difference
in terms of processing speed.

Online market offers several solutions for ML computing in cloud.
Most notable service providers for
MLaaS (Machine Learning as a Service)
are Google, Amazon, IBM, and Microsoft.
Differences of each service provider are listed in a table~\ref{fig:mlaas-comparison}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/mlaas-comparison}
    \caption{Machine learning as a Service comparison.~\cite{altexsoft2021mlaas}
    \label{fig:mlaas-comparison}}
\end{figure}

Amazon's new SageMaker service
has replaced the old Amazon Machine Learning service.
It is very much like Azure Machine Learning service
produced by Microsoft.
Azure, however,
has one major advantage
which is the ML Studio environment.
Most of the MLaaS provider's solutions
have some sort of no-code to low-code design features
which makes pipeline designing easy.
Azure ML Studio lets the developer design and deploy
full ML pipelines with drag-and-drop user interface.
Compared to SageMaker and Azure,
Google AI Platform is missing anomaly detection and ranking abilities.
IBM Watson has even less features,
as demonstrated in the table \ref{fig:mlaas-comparison}.~\cite{altexsoft2021mlaas}

%% ************************************************************************************************************

\subsection{Azure ML Studio}\label{subsec:bg-azure-ml-studio-algorithms}

Microsoft Azure offers a Machine Learning Studio environment
for easy ML pipeline designing.
ML Studio gives ML designer a possibility to
train algorithms and publish cloud endpoints
utilizing all Azure resources
connecting the power of ML
to all other Azure features
like data storages, IoT-services, and cloud computing.~\cite{altexsoft2021mlaas,microsoft2022azureml}

\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/azure-ml-studio-example}
    \caption{With drag-and-drop pipeline designer
    it is easy to get started with ML programming in Azure ML Studio,
        and visualizing the process helps understand all pipeline components
        and their relations to each other.
    \label{fig:azure-ml-studio-example}}
\end{figure}

Each component in pipeline can be tuned
to a certain extent.
ML Studio has a predefined set of ready algorithms to use.
Example of Azure ML Studio interface
is shown in figure \ref{fig:azure-ml-studio-example}.
Data to the ML Studio environment
can be imported from local storage,
but also from various other Azure services
such as storage accounts with table and blob data.
Trained ML pipeline can be inserted into wider operation chain
combining other Azure services to it.
This allows designer to use ML computing capabilities
with existing production environments
utilizing services like IoT, API, or Kubernetes.


%% ************************************************************************************************************

\subsection{Regression algorithm}\label{subsec:bg-regression-ml}
\todo{science and math behind regression algorithm}
Regression analysis is typical approach in statistical science.
It is used to find relationships with a set of variables.
%% TODO!
%% TODO: REFERENCES!
%% TODO: Mathematical background

%% ************************************************************************************************************

\subsection{PCA-based anomaly detection}\label{subsec:bg-pca-ada}
\todo{Explain PCA and mention other ADA algorithms}
%% TODO: rename subsection if more algorithms are introduced?

Principal Component Analysis, or PCA,
is a machine learning technique
used to analyze data and explain the variance inside it.~\cite{azure2022pca}



Other anomaly detection methods exist, but they are not supported by ML Studio
in a ready component level. %% TODO: check wording.
\todo{Something about Anomaly and Novelty detection differences?}
%% TODO: Mathematical background
%% TODO: How PCA works for anomaly detection? How is it used in it?
%% TODO: Differences for other anomaly detection methods? What other methods are there?

%% TODO!
\todo{placeholder picture. replace with mathematical explanation}
\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/pca-nutshell}
    \caption{PCA in a nutshell
        \label{fig:pca-nutshell}}
\end{figure}

%% ............................................................................................................

\subsubsection*{One-Class support vector machine}
%% TODO: This is also usable in azure, but not suitable in our case
%% TODO: make sure this is mentioned in places where "only one is pca" is considered

Azure ML Studio has also another anomaly detection algorithm to use.
This module is called One-Class Support Vector Machine.
In our case, however,
this module was not deemed suitable
as the documentation mentioned that
\enquote{The dataset that you use for training
can contain all or mostly normal cases.}
Because the content of the data used did not meet this requirement,
the usage of this component was decided to skip.

%% TODO: https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/one-class-support-vector-machine
%% TODO: add some reference to module

%% ************************************************************************************************************

\subsection{N-gram features and feature hashing}\label{subsec:bg-ngram-features}
%% TODO:
\todo{cover basic n-gram features and ML connection}

\toimhuom{unorganized text below:}
As stated before, %% TODO: state before!
features are the key elements in ML algorithm training.
As textual input does not have any meaning to machines as itself,
it is necessary to create a connection between words and features for algorithm.
In ML training, one typical approach is to convert textual input to numerical features.
For example, by creating a dictionary of words used in the input
and assigning each word an identification number,
we can express words as a count of certain words used.
In addition,
as words include meanings not only individually but also
with relation to each other and in their order, %% TODO: Some example!
we can add more information for the algorithm
by creating word pairs and groups in the dictionary.
These groups are referred as word grams,
where \textbf{n} in n-gram refers to the maximum number of words
in a group of consecutive words in the input sentence.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?
%% TODO: mathematical theories?

\todo{explain feature hashing component functionality briefly}
%% TODO: Clarifying example!

As the number of word grams in a dictionary can increase significantly
in complex input cases,
it is necessary to limit the resource usage by decreasing the features analyzed.
One way to do this is use feature hashing.
This means that instead of pure n-gram count
we use hashed value of several n-grams
thus reducing the amount of features.
As a drawback,
the amount of information might also get reduced as the data is "compressed"
but this way we can include more features for algorithm training
without significant resource demands.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?

%% ************************************************************************************************************

\subsection{Robotic process automation in Samlink}\label{subsec:bg-rpa-in-samlink}
\todo{Short explanation of RPA in Samlink, mostly to clear out the terms used in this study}

Robotic process automation, or RPA,
is used to automate mechanical tasks.
Usually it operates on the UI level
and can be used to repeat meaningful functions
instead of mechanical actions.
For example, with screen recording macros
only position at the screen and mechanical key pressing is recorded and repeated.
RPA automation, however,
is able to repeat the functionalities those actions trigger,
such as inputting text to a certain named field on the UI,
or logging in with given username and password
regardless of the location of those fields on the layout.

\todo{references!}

In Samlink RPA operations,
a central coordinating system called \enquote{Orchestrator}
supervises the RPA processes.

\todo{Add reference to hierarchy picture}

\todo{Dummy picture, replace with better}
\begin{figure}[htb]
    \centering
    \includegraphics[width=100mm,angle=270]{./appendices/rpa-hierarchy}
    \caption{TODO:! Dummy. To be replaced with proper picture.
    Hierarchy of RPA components explaining the terms and their relations
        \label{fig:rpa-hierarchy}}
\end{figure}



%% ************************************************************************************************************

\subsection{Data sensitivity}\label{subsec:bg-data-sensitivity}

During this study,
it was necessary to make sure no sensitive data
was moved out of the production environment.
This was mostly due to restrictions imposed by GDPR\@.
In order to maintain the data security,
data had to be anonymized
before it could be exported to the cloud environment.
After anonymization,
data would not include any information
that can be connected to real individuals.
Three different anonymization methods were considered,
which were pseudonymization, k-anonymization and full anonymization.

Pseudonymization refers to a method
where sensitive information is de-identified.
This means,
that each sensitive piece of information
is replaced with a decrypted value
so that no information is lost
but human cannot identify individuals
when reading the data.
Decryption and de-identification
could be reversed, \ie data could be re-identified,
with decryption key which tells computer
how to convert the replaced value back to the original form.
As machine learning algorithms do not care about the meanings
behind personal identification information,
such as phone numbers or addresses,
pseudonymization would preserve the information in the data unchanged
for ML algorithms to use so that no information would be lost.~\cite{noumeir2007pseudonymization}

As pseudonymization is reversible operation with encryption key,
it is not the safest way to anonymize the data
because encryption key leaking is always a risk.
K-anonymization is the next step to secure the data sensitivity.
Excluding all unique identifiers such as full name or social security number,
information like home street, age, workplace or last name
are not on their own enough to identify certain individual,
but combined they can single out a person.
K-anonymization is unreversable anonymization approach
where identifying information is generalized
to mask individuals into crowd.
With k-anonymization,
algorithm replaces single informative details
with more general variants,
for instance,
address to hometown or age to age range.
K-anonymization loses information
as it cannot be reversed.
If personal information is essential for the use case of ML algorithm,
this method weakens the algorithm results.~\cite{byun2007efficient}

However,
because it was determined that
individual information in the log data used in this study
was not relevant for connecting the log events to technical support ticket timestamps,
full anonymization was decided to execute on the log data.
This way,
each personal information was replaced with a general token
disclosing only what type of information (phone number, email address \etc) was anonymized.


\begin{table}[]\small
    \tabcolsep=0.11cm
    \begin{tabularx}{\textwidth}{|L{0.17\textwidth}|L{0.27\textwidth}|L{0.30\textwidth}|Y|}
        \hline
        \textbf{Info type} &
        \textbf{Example} &
        \textbf{Replaced value} &
        \textbf{Comment}
        \\ \hline
        Social security number &
        010190-0123 &
        10105051470101 &
        Includes '-','+' and 'a' format
        \\ \hline
        Email &
        author@thesis.fi &
        EmailAddress0101 &
        \\ \hline
        IBAN number &
        FI8612345600000123 &
        1010IBANnumber0101 &
        Only Finnish format
        \\ \hline
        BBAN number with dash &
        123456-123 &
        1010BBANnumber0101 &
        \\ \hline
        Phonenumber, international &
        +358501234567 &
        1010PhoneNumberInt0101 &
        With or without whitespaces
        \\ \hline
        Phonenumber, local &
        050-1234567 &
        1010980230101 &
        With or without whitespaces or dashes
        \\ \hline
        Business ID &
        1234567-8 &
        1010BusinessID0101 &
        Finnish format
        \\ \hline
        Business ID, international &
        FI12345678 &
        101086512350101 &
        \\ \hline
        Business ID, int. zero form &
        0012345678 &
        1010865123500101 &
        \\ \hline
        Credit card number &
        4920191061682346 &
        1010664900101 &
        \\ \hline
        Windows Identity &
        K123456 &
        1010WinID0101 &
        Used in company processes
        \\ \hline
        Address, common &
        Teekkarikuja 1 a 42 &
        1010AddressCommon0101 &
        Common street name endings
        \\ \hline
        Address, ZIP &
        Bulevardi 2 B 69, 00100 &
        1010AddressZip0101 &
        Disregards city name after ZIP
        \\ \hline
        Bank ID &
        12345678 &
        10108426100101 &
        Bank user ID
        \\ \hline
        BBAN without dash &
        12345600000123 &
        101088420101 &
        \\ \hline
        Artificial business ID &
        8123456789 &
        101086512354970101 &
        Used in RPA processes
        \\ \hline
    \end{tabularx}
    \caption{Information replaced with Regex search from log data.
    Data values are replaced with patterns with numbers or numbers and letters
    depending on the original format in the data.
    Patterns are formatted uniquely so that they can be recognized amongst the anonymized data,
    each starting with 1010 and ending with 0101,
    and having a typewise identifier in the middle.
    With numeric patters,
    numbers are selected as letter representations,
    like business ID = 8651235 (BUSINES)}
    \label{tab:regex-sensitive-info}
\end{table}



\begin{table}[]
    \begin{tabularx}{\textwidth}{|L{0.18\textwidth}|Y|}
        \hline
        \textbf{Info type} &
        \textbf{Regex} \\ \hline
        SSN &
        \tiny{\verb=?<![a-zA-Z0-9])[\d]{6}[-a+]?[\d]{3}[\w]{1}(?:0{0}|0{3})(?![a-zA-Z0-9])=}
         \\ \hline
        Email &
        \tiny{\verb=[^\`"\s]+@[\.\w-]*[\w]=}
         \\ \hline
        IBAN &
        \tiny{\verb&(?:(?<![a-zA-Z0-9])|(?<=\\\D))(?:FI|fi)(?: ?\d){16}(?![a-zA-Z0-9])&}
        \\ \hline
        BBANwithDash &
        \tiny{\verb=(?<![a-zA-Z0-9])[\d]{6}-[\d]{2,8}(?![a-zA-Z0-9])=}
         \\ \hline
        PhoneInt &
        \tiny{\verb=(?<![a-zA-Z0-9])\+358(?: ?\d){8,10}(?![a-zA-Z0-9])=}
         \\ \hline
        PhoneLoc &
        \tiny{\verb=(?<![a-zA-Z0-9-])[0][\d]{2,3}[ -]?(?: ?\d){6,8}(?![a-zA-Z0-9-])=}
         \\ \hline
        BusinessId &
        \tiny{\verb=(?<![a-zA-Z0-9])[\d]{7}-[\d]{1}(?![a-zA-Z0-9])=}
         \\ \hline
        BusinessIdInt &
        \tiny{\verb=(?<![a-zA-Z0-9])[a-zA-Z]{2}[\d]{8}(?![a-zA-Z0-9])=}
         \\ \hline
        BusinessIdIntZero &
        \tiny{\verb=(?<![a-zA-Z0-9])[0]{2}[\d]{8}(?![a-zA-Z0-9])=}
         \\ \hline
        CreditCard &
        \tiny{\verb=(?<![a-zA-Z0-9-.])[\d]{1}(?: ?\d){14,15}(?![a-zA-Z0-9-])=}
         \\ \hline
        WinId &
        \tiny{\verb=(?<![a-zA-Z0-9])[a-zA-Z]{1,2}[\d]{6}(?![a-zA-Z0-9])=}
         \\ \hline
        AddressCom &
        \tiny{\verb=[^\s""',.]* ?=\texttt{(katu|tie|kuja|polku|kaari|linja|raitti|rinne|penger|ranta|väylä|taival|tanhua|portti}
        \texttt{|veräjä|laita|reuna|syrjä|aukio|tori|laituri|tunneli)}\verb=[\d]{1,3}( ?[a-zA-Z.]{1,4} ?[\d]{0,3})?(?!\w)=}
        \\ \hline
        AddressZip &
        \tiny{\verb&(?<=\s)[\S]* [\d]{1,3}( ?[a-zA-Z.]{1,4} ?[\d]{0,3})?(\s|,\s)[\d]{5}(?!\w)&}
         \\ \hline
        BankId &
        \tiny{\verb=(?<![a-zA-Z0-9-])[\d]{8}(?![a-zA-Z0-9-])=}
         \\ \hline
        BBANnoDash &
        \tiny{\verb=(?<![a-zA-Z0-9])[\d]{14}(?![a-zA-Z0-9])=}
         \\ \hline
        ArtifBusinessId &
        \tiny{\verb=(?<![a-zA-Z0-9])[89]{1}[\d]{9}(?![a-zA-Z0-9])=}
         \\ \hline
    \end{tabularx}
    \caption{Regex search patterns for sensitive info finding.
    Most of the regex patterns start with negative lookbehind
    and end with negative lookahead
    so that found pattern is not part of another string.
    Order of the regex patterns is as shown on the table
    as some patterns gives overlapping matches.
    By searching shorter patterns before longer
    it is possible to recognize the type of anonymized information
    with higher reliability.}
    \label{tab:regex-sensitive-info2}
\end{table}

Data anonymization was executed in production environment
with PowerShell script.
Several predefined identification features were searched with
regular expression (or regex) patterns and replaced with
default keys.

%% TODO: WIP! Refer to tables!

%% ************************************************************************************************************

\subsection{Log data analyzing and anomaly detection with ML}\label{subsec:bg-log-data-analyzing-and-anomaly-detection-with-ml}

Using machine learning for log data analyzing
is not a new field of study.~\cite{rantala2019applying,allagi2019analysis,kondo2017early,cao2017machine}
Key issue tends to be the format of the log data
which shifts the dilemma to natural language processing.
Some studies also combine anomaly detection using machine learning
to log data analysis.~\cite{liu2019loganomaly, zhang2019robust}
Comparing existing studies to our case
raises at least one major suggestion for improvement:
log data refining.

When log data has consistent format,
multiple different algorithms can be utilized
for anomaly detection and log analyzing.
Log events can be clustered
and different types of events can be counted
if the amount of types is finite and known.~\cite{liu2019loganomaly}

If training data already have information
we wish to teach the algorithm to forecast (\ie data is labeled),
combining the results of the log data analysis to external features
is more feasible.
With labeled data,
supervised learning methods can improve the results of the algorithm forecast abilities.
~\cite{rantala2019applying}

As explained in the section~\ref{subsec:meth-efecte-ticket-data},
data features connected to anomaly detection results
are pure datetime values.
With more insight to ticket data properties,
ML algorithms could be able to extract more valuable information
from the log data.

%% ************************************************************************************************************

%% TODO: Workname. Check.
\subsection{Random delay in log event analyzing}\label{subsec:bg-random-delay}
\todo{Anything about the topic?}

Random delay in input data features
is not unusual aspect in time-series forecasting.
Time-series in ML context
refers to data features that varies over time
and is usually affected by past values.
As an example,
ML algorithm could try to predict future weather
based on measured temperature and air pressure.
Both these features change over time
and also affect to their own future values.

Random delay in such an example
could be due to some other local or global features
like
%% TODO: find proper exampe about time series forecasting and random delay!
%% TODO: REFERENCES

This study, however,
is not time-series
because the majority of the log rows
are not affected by previously logged events.
Random delay in this case
is caused by banking clerks
finding the issue and writing a technical support request.
This delay can span from hours to several days
depending on the weekday and time the issue occurred.

%% TODO: More to background?

As random delay of such
does not seem to be trivial to take into account
with ML algorithms, %% TODO: Any sources?
a simple method to solve this was used
which we call \enquote{time frame compression method}.
More about this approach is discussed in the section~\ref{subsec:pipe-timeframe-compression-and-statistics}

%% ************************************************************************************************************


\subsection{Hybrid machine learning approach in anomaly detection}\label{subsec:bg-hybrid-ml-approach-with-anomaly-detection}

Hybrid machine learning (HML)
refers to an ML technique
where two or more ML methods are combined
to overcome the limitations of
or to boost the estimation capabilities of
a single method alone.~\cite{Anifowose2020hml}
In this study,
we combine PCA-based anomaly detection algorithm
with regression algorithm
in order to amplify the prediction powers of our ML algorithm
when trying to determine the possible ticket count
based on log events.

Hybrid machine learning is not rare technique in ML field.~\cite{shon2007hybrid,tsai2010credit,mohan2019effective,
    hsieh2005hybrid,jain2007hybrid,kim2007hybrid,lee2002credit,malhotra2002differentiating}
%% TODO:
\todo{Something about the HML in ADA}

In order to clarify whether hybrid approach is suitable for the current study problem
we will compare the results of hybrid ML technique
with a single ML algorithm usage.


\todo{Include wireframe model about hybrid model}
%% TODO: Include wireframe model about hybrid model


%% TODO: Move to methods section?
With ML algorithm utilizing n-gram features combined with time frame compression
it is possible to get estimates
about the support tickets based on the log events.
It is not feasible to use anomaly detection on its own to do this
as plain sum of anomalies detected
is not correlating with tickets received.

We can, however,
amplify our ticket estimating algorithm with anomaly value features.
As we first count the anomaly numbers with anomaly detection algorithm %% TODO: better wording?
and their calculated statistical features with another algorithm,
like regression algorithm,
we get more relative information to use
when creating the final ticket number estimations. %% TODO: check sensability



\begin{itcomment}
    Hybrid ML as a term is used here to explain that we
    use two different ML algorithms in two separate phases.
    In first phase we try to give an anomaly certainty value for each log row
    using PCA-based anomaly detection component.
    In second phase we use this value as a feature to estimate ticket amount in time range
    by utilizing regression algorithm.

    Dual algorithm approach should not be unusual in ML field,
    but how existing studies or case examples relate to our way is uncertain.

    If nothing exists about the topic (at least nothing easily to be found)
    it should be worth to mention.
    But if there is a lot of case examples about this, it feels unnecessary
    to discus about it in more detail.
\end{itcomment}


\clearpage