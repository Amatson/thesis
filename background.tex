%!TEX root = thesis.tex
%% %% ***************** Background *****************

\section{Background}\label{sec:background}
%\section{Aikaisempi tutkimus}

Machine learning, or ML,
is a subcategory of the AI field and data science.
Typically, ML refers to
a set of technologies used to \enquote{build computers
that improve automatically through experience}.\cite{jordan2015machine}
This is generally considered a machine way
to simulate human learning process.
ML usage has become more common
and is nowadays widely used in many fields,
not just in general information technology and computer science.
This is because data can be gathered everywhere,
and where there is data to be processed,
ML can be there to process it.
Computer algorithms are able to find 
statistical correlation and patterns
from places overlooked by human mind,
or where amount of data is just too much 
for people to process.
This is why ML has proved its power
in various empirical science fields,
such as biology, cosmology or social science.\cite{jordan2015machine}

In this section, 
key concepts of ML are explained briefly
and several ML features are explored 
that are most relevant to this study.
We also discuss shortly about data sensitivity
and how it had to be addressed during this study.

%% ************************************************************************************************************


\subsection{Machine learning field}\label{subsec:bg-ml-field}
%% TODO: Open up ML algorithms and learning methods

Machine learning is built with algorithms
that operate

\subsubsection*{ML algorithms}
Algorithm means a finite sequence of (typically) mathematical operations
that are used to solve a specific problem, 
typically by repetition of some steps
until the problem resolves.\cite{merriam2022algorithm}
Algorithms are the main "magic" inside ML
where repeating patterns are searched from within the data
by iterating through all the data points.

In ML, 
algorithms can be generally divided into <four> categories
\todo{Check categories, create list or table}

Regression algorithms predict values
and are typically used with supervised learning.
\todo{Examples: House market price?}

Classification algorithms predict categories.
Depending on the algorithm,
they can predict between two or several categories.
\todo{Examples?}

Clustering algorithms use unsupervised learning
to find structures inside data.
\todo{Examples?}

Anomaly detection algorithms work also unsupervised
and try to find unusual or rare data points from data.
\todo{Examples?}

\todo{Something more generally about algorithms... Intro to next section (training).}

%% TODO: Reinforced learning? Should it be mentioned somehow/somewhere?

%% ************************************************************************************************************

\subsubsection*{ML training}
ML algorithms can be trained in two basic ways:
supervised and unsupervised learning.

In supervised learning,
algorithm is given data with ready answers on
how the data needs to be interpreted.
Algorithm then tries to figure out
how given data and the correct answers are related.

In unsupervised learning,
on the other hand,
algorithm does not get model data from which to train itself,
but instead it tries to find clusters or groups inside the data
that are linked together more closely than to other parts of data.
\todo{References!}
%% TODO: Other type of unsupervised learning algorithms exist

Typically,
when training an algorithm,
some predefined portion of the data
is used as training data.
\todo{Sources? Amounts?}
The rest is used to validate the results
so that validation data and training data do not overlap.
Instead, trained algorithm is given data it has not seen before
and the result it produces with it is then validated.
For example,
in supervised learning
the key values the algorithm is trained to find out
are hidden from the validation data.
The resulting values produced by the algorithm
are compared to those hidden values
and the difference between the estimate and the real value
can be used to determine how well the current algorithm compares to others.

\todo{Next one might change a bit depending on the actual results, which are still to be tested.}
However, in this study,
we are going to break that rule
about non-overlapping training and validation data.
The reason for this is explained further in section~\ref{subsec:meth-ml-pipeline})

%% ************************************************************************************************************

\subsection{Cloud ML platforms}\label{subsec:bg-cloud-ml-platforms}
\todo{Briefly about Azure, Google and AWS}

Machine learning algorithms are not light to operate.
Depending on the amount of data,
it can be a serious %% TODO: What? How big?
\todo{something something seriously big}

Especially with online applications
where real time analysis of new input data is required,
cloud computing resources can make a huge difference
in terms of processing speed. %% TODO: refs maybe?
\todo{referencing}

Online market offers several solutions for ML computing in cloud.
\todo{open up some facts about these. BRIEFLY}
Google

Amazon AWS(?)

Microsoft Azure\cite{altexsoft}

%% ************************************************************************************************************

\subsection{Azure ML Studio}\label{subsec:bg-azure-ml-studio-algorithms}

%% TODO!
\todo{More info about Azure ML studio}
\toimhuom{some unorganized text:}
Microsoft Azure offers a Machine Learning Studio environment
for easy ML pipeline designing.

Explain pipeline briefly.

ML Studio gives ML designer a possibility to
train algorithms and publish cloud endpoints
utilizing all Azure resources
connecting the power of ML
to all other Azure possibilities
like data storages, IoT-services and cloud computing.
%% TODO: IoT in abberviations?
%% TODO: references!

With drag-and-drop pipeline designer
it is easy to get started with ML programming,
and visualizing the process helps understand all pipeline components
and their relations to each other.
\todo{picture of azure pipeline}

\todo{Intro to next subsections}
Each component in pipeline can be tuned
to certain extent.
ML Studio has a predefined set of ready algorithms to use.
In this study we focus on  %% TODO: reconsider next
R-script execution component,
regression algorithm components,
PCA-based anomaly detection component,
N-Gram feature extraction component and
feature hashing component.


%% ************************************************************************************************************

\subsection{Regression algorithm}\label{subsec:bg-regression-ml}
\todo{science and math behind regression algorith}
Regression analysis is typical approach in statistical science.
It is used to find relationships with a set of variables.
%% TODO!
%% TODO: REFERENCES!
%% TODO: Mathematical background

%% ************************************************************************************************************

\subsection{PCA-based anomaly detection}\label{subsec:bg-pca-ada}
\todo{Explain PCA and mention other ADA algorithms}
%% TODO: rename subsection if more algorithms are introduced?

\toimhuom{unorganized text below:}

%% TODO: Reference: https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/pca-based-anomaly-detection

Principal Component Analysis, or PCA,
is an ML technique used to analyze data and explain the variance inside it.

Other anomaly detection methods exist, but they are not supported by ML Studio
in a ready component level. %% TODO: check wording.
\todo{Something about Anomaly and Novelty detection differences?}
%% TODO: Mathematical background
%% TODO: How PCA works for anomaly detection? How is it used in it?
%% TODO: Differences for other anomaly detection methods? What other methods are there?


\subsubsection*{One-Class support vector machine}
%% TODO: This is also usable in azure, but not suitable in our case
%% TODO: make sure this is mentioned in places where "only one is pca" is considered

Azure ML Studio has also another anomaly detection algorithm to use.
This module is called One-Class Support Vector Machine.
In our case, however,
this module was not deemed suitable
as the documentation mentioned that
\enquote{The dataset that you use for training
can contain all or mostly normal cases.}
Because the content of the data used did not meet this requirement,
the usage of this component was decided to skip.

%% TODO: https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/one-class-support-vector-machine
%% TODO: add some reference to module

%% ************************************************************************************************************

\subsection{N-gram features and feature hashing}\label{subsec:bg-ngram-features}
%% TODO:
\todo{cover basic n-gram features and ML connection}

\toimhuom{unorganized text below:}
As stated before, %% TODO: state before!
features are the key elements in ML algorithm training.
As textual input does not have any meaning to machines as itself,
it is necessary to create a connection between words and features for algorithm.
In ML training, one typical approach is to convert textual input to numerical features.
For example, by creating a dictionary of words used in the input
and assigning each word an identification number,
we can express words as a count of certain words used.
In addition,
as words include meanings not only individually but also
with relation to each other and in their order, %% TODO: Some example!
we can add more information for the algorithm
by creating word pairs and groups in the dictionary.
These groups are referred as word grams,
where \textbf{n} in n-gram refers to the maximum number of words
in a group of consecutive words in the input sentence.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?
%% TODO: mathematical theories?

\todo{explain feature hashing component functionality briefly}
%% TODO: Clarifying example!

As the number of word grams in a dictionary can increase significantly
in complex input cases,
it is necessary to limit the resource usage by decreasing the features analyzed.
One way to do this is use feature hashing.
This means that instead of pure n-gram count
we use hashed value of several n-grams
thus reducing the amount of features.
As a drawback,
the amount of information might also get reduced as the data is "compressed"
but this way we can include more features for algorithm training
without significant resource demands.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?

%% ************************************************************************************************************

\subsection{Data sensitivity}\label{subsec:bg-data-sensitivity}
\todo{some basic stuff about anonymization, data sensitivity and data protection}
\todo{Just practicalities, not much about theory or background}

During this study,
it was necessary to make sure no sensitive data
was moved out of the production environment.
This was mostly due to regulations described in GDPR. %% TODO: wording? More?

Data anonymization was executed in production environment
with Powershell script.
Several predefined identification features were searched with
regular expression (or regex), patterns and replaced with
default keys.
%% TODO: examples
%% TODO: Sources!
%% TODO: How does anonymization affect to ML algorithm functionalities/usages?

%% ************************************************************************************************************

\subsection{Log data analyzing with ML}\label{subsec:bg-log-data-analyzing-with-ml}
\todo{Some research should exist}
TODO: In this section, we look what studies and cases of ML log data analyzing
exists in the IT field.
Only briefly, nothing too deep


%% ************************************************************************************************************

%% TODO: Workname. Check.
\subsection{Random delay in log event analyzing}\label{subsec:bg-random-delay}
\todo{Anything about the topic?}

Random delay in input data features
is not unusual aspect in time-series forecasting.
Time-series in ML context
refers to data features that varies over time
and is usually affected by past values.
As an example,
ML algorithm could try to predict future weather
based on measured temperature and air pressure.
Both these features change over time
and also affect to their own future values.

Random delay in such an example
could be due to some other local or global features
like
%% TODO: find proper exampe about time series forecasting and random delay!
%% TODO: REFERENCES

This study, however,
is not time-series
because the majority of the log rows
are not affected by previously logged events.
Random delay in this case
is caused by banking clerks
finding the issue and writing a technical support request.
This delay can span from hours to several days
depending on the weekday and time the issue occurred.

%% TODO: More to background?

%% TODO: Move  below to methods section!
As random delay of such
does not  seem to be trivial to take into account
with ML algorithms, %% TODO: Any sources?
a simple method to solve this was used
we call \enquote{time frame compression method}.
%% TODO: Is there anything to refer to???
This means that
in order to eliminate the effects of random delay
we compress some features in certain time frame
which is at least as long as the longest estimated delay.
Simply put,
as we count possible anomalies during one hour of log,
we cannot compare this number to actual tickets received
at the same hour or the next.
What we can do,
with time frame compression,
is that we count some statistical values of anomaly estimates,
for example, the mean and median values of a week,
and then compare these numbers with the tickets received
during the same week.


%% ************************************************************************************************************


\subsection{Hybrid machine learning approach in anomaly detection}\label{subsec:bg-hybrid-ml-approach-with-anomaly-detection}

Hybrid machine learning (HML)
refers to an ML technique
where two or more ML methods are combined
to overcome the limitations of
or to boost the estimation capabilities of
a single method alone.\cite{Anifowose2020hml}
In this study,
we combine PCA-based anomaly detection algorithm
with regression algorithm
in order to amplify the prediction powers of our ML algorithm
when trying to determine the possible ticket count
based on log events.

Hybrid machine learning is not rare technique in ML field.\cite{shon2007hybrid,tsai2010credit,mohan2019effective,
    hsieh2005hybrid,jain2007hybrid,kim2007hybrid,lee2002credit,malhotra2002differentiating}
%% TODO:
\todo{Something about the HML in ADA}

In order to clarify whether hybrid approach is suitable for the current study problem
we will compare the results of hybrid ML technique
with a single ML algorithm usage.


%% TODO: Include wireframe model about hybrid model


%% TODO: Move to methods section?
With ML algorithm utilizing n-gram features combined with time frame compression
it is possible to get estimates
about the support tickets based on the log events.
It is not feasible to use anomaly detection on its own to do this
as plain sum of anomalies detected
is not correlating with tickets received.

We can, however,
amplify our ticket estimating algorithm with anomaly value features.
As we first count the anomaly numbers with anomaly detection algorithm %% TODO: better wording?
and their calculated statistical features with another algorithm,
like regression algorithm,
we get more relative information to use
when creating the final ticket number estimations. %% TODO: check sensability



\begin{itcomment}
    Hybrid ML as a term is used here to explain that we
    use two different ML algorithms in two separate phases.
    In first phase we try to give an anomaly certainty value for each log row
    using PCA-based anomaly detection component.
    In second phase we use this value as a feature to estimate ticket amount in time range
    by utilizing regression algorithm.

    Dual algorithm approach should not be unusual in ML field,
    but how existing studies or case examples relate to our way is uncertain.

    If nothing exists about the topic (at least nothing easily to be found)
    it should be worth to mention.
    But if there is a lot of case examples about this, it feels unnecessary
    to discus about it in more detail.
\end{itcomment}


\clearpage