%!TEX root = thesis.tex
%% %% ***************** Background *****************

%% ************************************************ 2 ************************************************

\section{Background}\label{sec:background}
%\section{Aikaisempi tutkimus}

Machine learning, or ML,
is a subcategory of the AI field and data science.
Typically, ML refers to
a set of technologies used to \enquote{build computers
that improve automatically through experience}.~\cite{jordan2015machine}
This is generally considered a machine way
to simulate human learning process.
ML usage has become more common
and is nowadays widely used in many fields,
not just in general information technology and computer science.
This is because data can be gathered from anywhere,
and where there is data to be processed,
ML can be there to process it.
Computer algorithms are able to find 
statistical correlation and patterns
from places overlooked by human mind,
or where amount of data is just too much 
for people to process.
This is why ML has proved its power
in various empirical science fields,
such as biology, cosmology or social science.~\cite{jordan2015machine}

In this section, 
key concepts of ML are explained briefly
and several ML features are explored 
that are most relevant to this study.
We also discuss shortly about data sensitivity
and how it had to be addressed during this study.

%% ************************************************************************************************************


\subsection{Machine learning algorithms and training}\label{subsec:bg-machine-learning}

Algorithm means a finite sequence of (typically) mathematical operations
that are used to solve a specific problem,
generally by repetition of some steps
until the problem resolves.~\cite{merriam2022algorithm}
Algorithms are the main component inside machine learning.
By iterating through all the data points
algorithm is able to, for example,
find repeating patterns,
mathematical or logical connections,
or unusual anomalies that would be seemingly normal for human eye.

Algorithms operate on set of rules and parameters
In order to utilize an algorithm to solve a problem,
algorithm is first trained by tuning these parameters
to fit the current case.
Usually,
ML algorithms can be trained in three ways:
supervised, unsupervised, and reinforced learning.~\cite{jordan2015machine}
Even more training methods exist
that usually combine those mentioned.~\cite{ayodele2010types, mahesh2020machine}
For the sake of simplicity,
we focus on those three main methods.

In \textbf{supervised learning},
algorithm is given data with ready answers on
how the data needs to be interpreted.
Algorithm then tries to figure out the rules behind
how given data and the correct answers are related.~\cite{ayodele2010types}
In \textbf{unsupervised learning},
on the other hand,
algorithm does not get model data from which to train itself,
but instead it tries to find clusters or groups inside the data
that are linked together more closely than to other data points.~\cite{winky}
\textbf{Reinforced learning} refers to a method
where a computer program is given a goal
and provided feedback as a reward.
This reward is what program aims to maximize
by adjusting given parameters.~\cite{ayodele2010types}

In ML,
there are multiple algorithms to solve different problems
and no jack-of-all-trades algorithm exists.
Each algorithm is suitable for certain type of problem.
To simplify,
algorithms are usually divided into three or four categories
based on the problem type.~\cite{vickery2019mltypes}

\textbf{Regression algorithms} predict values
and are typically used with supervised learning.
Usual example of regression problem
is house price prediction
using typical house features
such as building
year,location, number of rooms \etc.
With these varying features
the algorithm then gives each feature a weight value
which determine the final price of the house.~\cite{vickery2019mltypes}

\textbf{Classification algorithms} predict categories
and are also used most commonly with supervised learning.
Depending on the algorithm,
they can predict between two or several categories.
Examples of classification problems
could be spam mail identification with two class classification,
or flower species recognition from images with multiclass classification.~\cite{vickery2019mltypes}

\textbf{Clustering algorithms} use unsupervised learning
to find structures inside data.
This is done,
for instance,
by first providing the amount of clusters to search to algorithm,
which then calculates a center point for each cluster
so that they are as far away from each other as possible
while data points surrounding each center are as close to each other as possible.~\cite{mahesh2020machine}
This could be used,
for example,
to find meaningful customer segments from transaction data
in order to improve targeted advertising.~\cite{chen2017purtreeclust}

\textbf{Dimension reduction algorithms} are a separate type of algorithms used with unsupervised learning,
but they are usually combined with other algorithms
to solve the main problem.
With dimension reduction,
main algorithm calculations are streamlined by first reducing the amount of feature dimensions.~\cite{li2017mlalgorithm}

These four ML problem types
and most known algorithms of each type
are shown in the graphic~\ref{fig:ml-algorithm-cheatsheet}.


\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/machine-learning-cheet-sheet-2}
    \caption{Machine learning cheatsheet for algorithm choosing\cite{li2017mlalgorithm}
    \label{fig:ml-algorithm-cheatsheet}}
\end{figure}

This study focuses on anomaly detection,
which, roughly simplified, is a clustering problem
where anomalies are rare incidents outside common clusters.
However,
in this study we utilize PCA-based anomaly detection algorithm,
where PCA refers to Principal Component Analysis,
and which is a dimension reduction algorithm.~\cite{li2017mlalgorithm}
More about PCA
is discussed later in this section.
In addition,
we aim to find a connection between anomalies and incident tickets
by their amount in a timeframe,
which makes the topic in the end a regression problem.

Typically,
when training an algorithm,
some predefined portion of the data
is used as training data.
The rest is used to validate the results
so that validation data and training data do not overlap.
Instead, trained algorithm is given data it has not seen before
and the result it produces with it is then validated.~\cite{baheti2022datasplit}
For example,
in supervised learning
the key values the algorithm is trained to find out
are hidden from the validation data.
The resulting values produced by the algorithm
are compared to those hidden values
and the difference between the estimate and the real value
can be used to determine how well the current trained algorithm compares to others.
However, in this study,
we are going to break that rule
about non-overlapping training and validation data.
The reason for this is explained further in section~\ref{subsec:pipe-unconventional-training}.

%% ************************************************************************************************************

\subsection{Cloud ML platforms}\label{subsec:bg-cloud-ml-platforms}

Machine learning algorithms are not light to operate.
ML is at its best with big data
where amount of data points
makes it easier for algorithms
to find repeating patterns more reliably.~\cite{zhou2017machine}
Data amount, however,
requires huge resources in terms of memory and computing power.
Especially with online applications
where real time analysis of new input data is required
with small latency,
cloud computing can make a big difference
in terms of processing speed.

Online market offers several solutions for ML computing in cloud.
Most notable service providers for
MLaaS (Machine Learning as a Service)
are Google, Amazon, IBM, and Microsoft.
Differences of each service provider are listed in a table~\ref{fig:mlaas-comparison}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/mlaas-comparison}
    \caption{Machine learning as a Service comparison.~\cite{altexsoft2021mlaas}
    \label{fig:mlaas-comparison}}
\end{figure}

Amazon's new SageMaker service
has replaced the old Amazon Machine Learning service,
and is very much like Azure Machine Learning service
produced by Microsoft.
Compared to SageMaker and Azure,
Google AI Platform is missing anomaly detection and ranking abilities.
IBM Watson has even less features,
as demonstrated in the table~\ref{fig:mlaas-comparison}.~\cite{altexsoft2021mlaas}

Azure, however,
has one major advantage compared to SageMaker and other competitors,
which is the UI environment of ML Studio.
Most of the MLaaS providers' solutions
have some sort of no-code to low-code design features
which makes pipeline designing easy.
Azure ML Studio lets the developer design and deploy
full ML pipelines with drag-and-drop user interface.~\cite{altexsoft2021mlaas,microsoft2022azureml}

\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/azure-ml-studio-example}
    \caption{With drag-and-drop pipeline designer
    it is easy to get started with ML programming in Azure ML Studio,
    and visualizing the process helps understand all pipeline components
    and their relations to each other.
    \label{fig:azure-ml-studio-example}}
\end{figure}

Each component in pipeline designer can be tuned
to a certain extent.
ML Studio has a predefined set of ready algorithms to use.
Example of Azure ML Studio interface
is shown in figure~\ref{fig:azure-ml-studio-example}.
Data to the ML Studio environment
can be imported from local storage,
but also from various other Azure services
such as storage accounts with table and blob data.
Trained ML pipeline can be published as a cloud endpoint
and inserted into wider operation chain
combining other Azure services to it,
like data storages and cloud computing resources.
This allows designer to use ML computing capabilities
with existing production environments
utilizing services such as IoT, API, or Kubernetes.


%% ************************************************************************************************************

\subsection{Regression analysis}\label{subsec:bg-regression-ml}

Regression analysis is typical approach in statistical science,
and thus, in machine learning too.
Algorithms based on regression analysis
are used to find relationships between a set of variables,
providing the means to \enquote{predict values of one variable
when given values of the others}
making them a fundamental component in ML field.~\cite{merriam2022regression}

Regression algorithms intend to create a mathematical model
that explains the data.
This usually means an algebraic equation
which can be generalized in the following form,
\begin{equation}
    \label{eqn:general-regression-model}
    Y = f(X_{m},\beta_{p})+\epsilon
\end{equation}
where $f$ is some function of $m$ independent variables $(X_{m})$,
and $p$ coefficients $(\beta_{p})$.
$X_{m}$ can be opened as $X_{1},...,X_{m}$,
and $\beta_{p}$ as $\beta_{1},...,\beta_{p}$.
Note, that $m$ and $p$ do not have to be equal.
A single $X_{i}$ refers to a value of $i$th data row.
The $\epsilon$ refers to error term.
The goal is to determine the coefficients $\beta_{p}$
in order to find a model that explains the data.~\cite{freund2006regression}

As an example,
one of the best known principles for coefficient value solving
is the least square principle,
which aims to minimize the sum of squared errors (SSE).
Now,
the smaller the $SSE$ is,
the closer each data point is to the suggested model,
and the better the model explains the data
and can be used to make predictions.
The equation to solve in the least square principle
derived from the generalized form,
\begin{equation}
    \label{eqn:sum-of-squared-errors}
    SSE = \Sigma\epsilon^{2} = \Sigma[(Y-f(X_{1},...,X_{m},\beta_{1},...,\beta_{p}))]^{2}.
\end{equation}
Exact solution for this does not usually exist
as there can be more coefficients than independent variables $(p>m))$,
and minimizing the $SSE$ does not result into linear equation.
This is typically the case in ML,
and thus the model must be found
by iterative search process.
As mentioned in the section~\ref{subsec:bg-machine-learning},
this is what algorithms are build for.

To improve the efficiency of algorithm calculations,
general regression model can be converted into a matrix form.
The same function in equation~\ref{eqn:general-regression-model}
can thus be represented as,

\begin{equation}
    Y = XB+E
\end{equation}
where $Y$ is an $n\times1$ matrix of values in the data,
$X$ is an $n\times(m+1)$ matrix of independent variables,
$B$ is an $(m+1)\times1$ matrix of unknown coefficient parameters,
and finally, $E$ is an $n\times1$ matrix of error parameters.
Thus, the equation of squared errors (equation~\ref{eqn:sum-of-squared-errors})
that is to be minimized can be written as
\begin{equation}
    E'E = (Y-XB)'(Y-XB) = Y'Y-2B'X'Y+B'X'XB.
\end{equation}
In order to minimize this, we can take the derivative of matrix $B$,
\begin{equation}
    \frac{\partial(E'E)}{\partial B} = -2X'Y + 2X'XB
\end{equation}
and equating to zero gives
\begin{equation}
    (X'X)\hat{B} = X'Y.
\end{equation}
The solutions of coefficient parameters are thus
\begin{equation}
    \hat{B} = (X'X)^{-1}X'Y.
\end{equation}

Ultimately,
there usually is no one perfect model to describe real world data.
The function $f$ in the general regression model
can describe the relation of $X_{i}$ and $\beta_{i}$ as product of both terms.
This would mean, that
\begin{equation}
    f(X_{1},...,X_{m},\beta_{1},...,\beta_{m})=\beta_{0}+\beta_{1}x_{1}+...+\beta_{m}x_{m}
\end{equation}
which would be a linear regression model.
Example of an algorithm using this method
is visualized in the picture~\ref{fig:linear-regression-example}.
However,
real world problems cannot always be explained with linear model.
Fitting a polynomial curve to the data
may improve the results of regression to a certain degree,
but it also has its limits.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{./appendices/linear-regression}
    \caption{Linear least squares algorithm.
    Algorithm tries to fit a linear model (red line) on the data points,
        represented by the black dots,
        by minimizing the difference between all the data points
        and the model estimate.~\cite{stulp2015many}
        \label{fig:linear-regression-example}}
\end{figure}

Consider an algorithm that tries to keep a car
driving on a straight road in its lane.
The model output is how much we should turn the steering wheel
based on how much we are off from the straight line.
We might have constant parameters such as
wind effect, continuous error of the axes, and tilt of the road.
However,
we also have parameters that change over time
and are dependent from other factors,
such as the weight of the car,
and temperature and wear of the tires.
So, with the same location at the road,
same car and same weather conditions,
our result could be different
depending on how long has been driven before that current moment
which affects the tire temperature and thus pressure.
Finding a model that fits the best to the data
depends also on the type of problem.
There are numerous algorithms to suit different situations,
and usually the best algorithm can be found
only by trial and error.


%% ************************************************************************************************************

\subsection{PCA-based anomaly detection}\label{subsec:bg-pca-ada}
\todo{Explain PCA and mention other ADA algorithms}
%% TODO: rename subsection if more algorithms are introduced?

Principal Component Analysis, or PCA,
is a machine learning technique
used to analyze data and explain the variance inside it.~\cite{azure2022pca}



Other anomaly detection methods exist, but they are not supported by ML Studio
in a ready component level. %% TODO: check wording.
\todo{Something about Anomaly and Novelty detection differences?}
%% TODO: Mathematical background
%% TODO: How PCA works for anomaly detection? How is it used in it?
%% TODO: Differences for other anomaly detection methods? What other methods are there?

%% TODO!
\todo{placeholder picture. replace with mathematical explanation}
\begin{figure}[htb]
    \centering
    \includegraphics[width=150mm]{./appendices/pca-nutshell}
    \caption{PCA in a nutshell
        \label{fig:pca-nutshell}}
\end{figure}

%% ............................................................................................................

\subsubsection*{One-Class support vector machine}
%% TODO: This is also usable in azure, but not suitable in our case
%% TODO: make sure this is mentioned in places where "only one is pca" is considered

Azure ML Studio has,
or used to have,
also another anomaly detection algorithm to use.
This module is called One-Class Support Vector Machine.
However,
this module was not usable in the renewed ML Studio environment,
but was only usable in the \textit{classic} Azure ML Studio.
In addition,
this module was not deemed suitable in our case,
as the documentation mentioned that
\enquote{The dataset that you use for training
can contain all or mostly normal cases.}
Because the content of the data used did not meet this requirement,
the usage possibilities of this component were not decided to investigate.~\cite{azure2021oneclasssvm}

%% ************************************************************************************************************

\subsection{N-gram features and feature hashing}\label{subsec:bg-ngram-features-and-hashing}
%% TODO:
\todo{cover basic n-gram features and ML connection}

\toimhuom{unorganized text below:}
As stated before, %% TODO: state before!
features are the key elements in ML algorithm training.
As textual input does not have any meaning to machines as itself,
it is necessary to create a connection between words and features for algorithm.
In ML training, one typical approach is to convert textual input to numerical features.
For example, by creating a dictionary of words used in the input
and assigning each word an identification number,
we can express words as a count of certain words used.
In addition,
as words include meanings not only individually but also
with relation to each other and in their order, %% TODO: Some example!
we can add more information for the algorithm
by creating word pairs and groups in the dictionary.
These groups are referred as word grams,
where \textbf{n} in n-gram refers to the maximum number of words
in a group of consecutive words in the input sentence.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?
%% TODO: mathematical theories?

\todo{explain feature hashing component functionality briefly}
%% TODO: Clarifying example!

As the number of word grams in a dictionary can increase significantly
in complex input cases,
it is necessary to limit the resource usage by decreasing the features analyzed.
One way to do this is use feature hashing.
This means that instead of pure n-gram count
we use hashed value of several n-grams
thus reducing the amount of features.
As a drawback,
the amount of information might also get reduced as the data is "compressed"
but this way we can include more features for algorithm training
without significant resource demands.

%% TODO: check above
%% TODO: REFERENCES!
%% TODO: More?

%% ************************************************************************************************************

\subsection{Robotic process automation}\label{subsec:bg-rpa}
\todo{Short explanation of RPA in Samlink, mostly to clear out the terms used in this study}

Robotic process automation, or RPA,
is used to automate mechanical tasks.
Usually it operates on the UI level
and can be used to repeat meaningful functions
instead of mechanical actions.
For example, with screen recording macros
only position at the screen and mechanical key pressing is recorded and repeated.
RPA automation, however,
is able to repeat the functionalities those actions trigger,
such as inputting text to a certain named field on the UI,
or logging in with given username and password
regardless of the location of those fields on the layout.

\todo{references!} %% TODO!!

In Samlink RPA operations,
a central coordinating system called \enquote{Orchestrator}
supervises the RPA processes.

\todo{Add reference to hierarchy picture}

\todo{Dummy picture, replace with better}
\begin{figure}[htb]
    \centering
    \includegraphics[width=100mm,angle=270]{./appendices/rpa-hierarchy}
    \caption{TODO:! Dummy. To be replaced with proper picture.
    Hierarchy of RPA components explaining the terms and their relations
        \label{fig:rpa-hierarchy}}
\end{figure}



%% ************************************************************************************************************

\subsection{Data sensitivity}\label{subsec:bg-data-sensitivity}

During this study,
it was necessary to make sure no sensitive data
was moved out of the production environment.
This was mostly due to restrictions imposed by GDPR\@.
In order to maintain the data security,
data had to be anonymized
before it could be exported to the cloud environment.
After anonymization,
data would not include any information
that can be connected to real individuals.
Three different anonymization methods were considered,
which were pseudonymization, k-anonymization and full anonymization.

Pseudonymization refers to a method
where sensitive information is de-identified.
This means,
that each sensitive piece of information
is replaced with a decrypted value
so that no information is lost
but human cannot identify individuals
when reading the data.
Decryption and de-identification
could be reversed, \ie data could be re-identified,
with decryption key which tells computer
how to convert the replaced value back to the original form.
As machine learning algorithms do not care about the meanings
behind personal identification information,
such as phone numbers or addresses,
pseudonymization would preserve the information in the data unchanged
for ML algorithms to use so that no information would be lost.~\cite{noumeir2007pseudonymization}

As pseudonymization is reversible operation with encryption key,
it is not the safest way to anonymize the data
because encryption key leaking is always a risk.
K-anonymization is the next step to secure the data sensitivity.
Excluding all unique identifiers such as full name or social security number,
information like home street, age, workplace or last name
are not on their own enough to identify certain individual,
but combined they can single out a person.
K-anonymization is unreversable anonymization approach
where identifying information is generalized
to mask individuals into crowd.
With k-anonymization,
algorithm replaces single informative details
with more general variants,
for instance,
address to hometown or age to age range.
K-anonymization loses information
as it cannot be reversed.
If personal information is essential for the use case of ML algorithm,
this method weakens the algorithm results.~\cite{byun2007efficient}

Eventually,
due to high customer data sensitivity and strict data safety policies,
it was determined that individual information in the log data used in this study
was not relevant for connecting the log events to technical support ticket timestamps,
and full anonymization was decided to execute on the log data.
This way,
each personal information was replaced with a general token
disclosing only what type of information (phone number, email address \etc) was anonymized.
Frankly,
it was not certain if identifiable personal data had improved algorithm results,
but because corresponding support ticket data
was stripped from all other information except timestamps,
any possible connections between personal data in logs and in tickets
were lost in any case.

Data anonymization was executed in production environment
with PowerShell script.
Several predefined identification features were searched with
regular expression (or regex) patterns and replaced with
default keys.
Anonymization scripts and data format
is described more in the section~\ref{subsec:meth-data-anonymization}.


%% ************************************************************************************************************

\subsection{Log data analyzing and anomaly detection with ML}\label{subsec:bg-log-data-analyzing-and-anomaly-detection-with-ml}

Using machine learning for log data analyzing
is not a new field of study.~\cite{rantala2019applying,allagi2019analysis,kondo2017early,cao2017machine}
Key issue tends to be the format of the log data
which shifts the dilemma to natural language processing.
Some studies also combine anomaly detection using machine learning
to log data analysis.~\cite{liu2019loganomaly, zhang2019robust}
Comparing existing studies to our case
raises at least one major suggestion for improvement:
log data refining.

When log data has consistent format,
multiple different algorithms can be utilized
for anomaly detection and log analyzing.
Log events can be clustered
and different types of events can be counted
if the amount of types is finite and known.~\cite{liu2019loganomaly}

If training data already have information
we wish to teach the algorithm to forecast (\ie data is labeled),
combining the results of the log data analysis to external features
is more feasible.
With labeled data,
supervised learning methods can improve the results of the algorithm forecast abilities.
~\cite{rantala2019applying}

As explained in the section~\ref{subsec:meth-efecte-ticket-data},
data features connected to anomaly detection results
are pure datetime values.
With more insight to ticket data properties,
ML algorithms could be able to extract more valuable information
from the log data.


%% ************************************************************************************************************


\subsection{Hybrid machine learning approach in anomaly detection}\label{subsec:bg-hybrid-ml-approach-with-anomaly-detection}

Hybrid machine learning (HML)
refers to an ML technique
where two or more ML methods are combined
to overcome the limitations of
or to boost the estimation capabilities of
a single method alone.~\cite{Anifowose2020hml}
In this study,
we combine PCA-based anomaly detection algorithm
with regression algorithm
in order to amplify the prediction powers of our ML algorithm
when trying to determine the possible ticket count
based on log events.

Hybrid machine learning is not rare technique in ML field.~\cite{shon2007hybrid,tsai2010credit,mohan2019effective,
    hsieh2005hybrid,jain2007hybrid,kim2007hybrid,lee2002credit,malhotra2002differentiating}
%% TODO:
\todo{Something about the HML in ADA}

In order to clarify whether hybrid approach is suitable for the current study problem
we will compare the results of hybrid ML technique
with a single ML algorithm usage.


\todo{Include wireframe model about hybrid model}
%% TODO: Include wireframe model about hybrid model


%% TODO: Move to methods section?
With ML algorithm utilizing n-gram features combined with time frame compression
it is possible to get estimates
about the support tickets based on the log events.
It is not feasible to use anomaly detection on its own to do this
as plain sum of anomalies detected
is not correlating with tickets received.

We can, however,
amplify our ticket estimating algorithm with anomaly value features.
As we first count the anomaly numbers with anomaly detection algorithm %% TODO: better wording?
and their calculated statistical features with another algorithm,
like regression algorithm,
we get more relative information to use
when creating the final ticket number estimations. %% TODO: check sensability



\begin{itcomment}
    Hybrid ML as a term is used here to explain that we
    use two different ML algorithms in two separate phases.
    In first phase we try to give an anomaly certainty value for each log row
    using PCA-based anomaly detection component.
    In second phase we use this value as a feature to estimate ticket amount in time range
    by utilizing regression algorithm.

    Dual algorithm approach should not be unusual in ML field,
    but how existing studies or case examples relate to our way is uncertain.

    If nothing exists about the topic (at least nothing easily to be found)
    it should be worth to mention.
    But if there is a lot of case examples about this, it feels unnecessary
    to discus about it in more detail.
\end{itcomment}


\clearpage